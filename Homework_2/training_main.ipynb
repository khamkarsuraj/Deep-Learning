{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad47cabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e6360df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess():\n",
    "    filepath = ''\n",
    "    with open(filepath + 'training_label.json', 'r') as f:\n",
    "        file = json.load(f)\n",
    "\n",
    "    word_count = {}\n",
    "    for d in file:\n",
    "        for s in d['caption']:\n",
    "            word_sentence = re.sub('[.!,;?]]', ' ', s).split()\n",
    "            for word in word_sentence:\n",
    "                word = word.replace('.', '') if '.' in word else word\n",
    "                if word in word_count:\n",
    "                    word_count[word] += 1\n",
    "                else:\n",
    "                    word_count[word] = 1\n",
    "\n",
    "    word_dict = {}\n",
    "    for word in word_count:\n",
    "        if word_count[word] > 4:\n",
    "            word_dict[word] = word_count[word]\n",
    "    useful_tokens = [('<PAD>', 0), ('<SOS>', 1), ('<EOS>', 2), ('<UNK>', 3)]\n",
    "    i2w = {i + len(useful_tokens): w for i, w in enumerate(word_dict)}\n",
    "    w2i = {w: i + len(useful_tokens) for i, w in enumerate(word_dict)}\n",
    "    for token, index in useful_tokens:\n",
    "        i2w[index] = token\n",
    "        w2i[token] = index\n",
    "        \n",
    "    return i2w, w2i, word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "969ce992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_split(sentence, word_dict, w2i):\n",
    "    sentence = re.sub(r'[.!,;?]', ' ', sentence).split()\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i] not in word_dict:\n",
    "            sentence[i] = 3\n",
    "        else:\n",
    "            sentence[i] = w2i[sentence[i]]\n",
    "    sentence.insert(0, 1)\n",
    "    sentence.append(2)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e9c509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(label_file, word_dict, w2i):\n",
    "    label_json = '' + label_file\n",
    "    annotated_caption = []\n",
    "    with open(label_json, 'r') as f:\n",
    "        label = json.load(f)\n",
    "    for d in label:\n",
    "        for s in d['caption']:\n",
    "            s = s_split(s, word_dict, w2i)\n",
    "            annotated_caption.append((d['id'], s))\n",
    "    return annotated_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4d02236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avi(files_dir):\n",
    "    avi_data = {}\n",
    "    training_feats = '' + files_dir\n",
    "    files = os.listdir(training_feats)\n",
    "    for file in files:\n",
    "        value = np.load(os.path.join(training_feats, file))\n",
    "        avi_data[file.split('.npy')[0]] = value\n",
    "    return avi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11c0b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch(data):\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    avi_data, captions = zip(*data) \n",
    "    avi_data = torch.stack(avi_data, 0)\n",
    "\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "    return avi_data, targets, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ab71da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class training_data(Dataset):\n",
    "    def __init__(self, label_file, files_dir, word_dict, w2i):\n",
    "        self.label_file = label_file\n",
    "        self.files_dir = files_dir\n",
    "        self.word_dict = word_dict\n",
    "        self.avi = avi(label_file)\n",
    "        self.w2i = w2i\n",
    "        self.data_pair = annotate(files_dir, word_dict, w2i)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_pair)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        assert (idx < self.__len__())\n",
    "        avi_file_name, sentence = self.data_pair[idx]\n",
    "        data = torch.Tensor(self.avi[avi_file_name])\n",
    "        data += torch.Tensor(data.size()).random_(0, 2000)/10000.\n",
    "        return torch.Tensor(data), torch.Tensor(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64f79664",
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_data(Dataset):\n",
    "    def __init__(self, test_data_path):\n",
    "        self.avi = []\n",
    "        files = os.listdir(test_data_path)\n",
    "        for file in files:\n",
    "            key = file.split('.npy')[0]\n",
    "            value = np.load(os.path.join(test_data_path, file))\n",
    "            self.avi.append([key, value])\n",
    "    def __len__(self):\n",
    "        return len(self.avi)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.avi[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "449e6705",
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(attention, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear1 = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.to_weight = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden_state, encoder_outputs):\n",
    "        batch_size, seq_len, feat_n = encoder_outputs.size()\n",
    "        hidden_state = hidden_state.view(batch_size, 1, feat_n).repeat(1, seq_len, 1)\n",
    "        matching_inputs = torch.cat((encoder_outputs, hidden_state), 2).view(-1, 2*self.hidden_size)\n",
    "\n",
    "        x = self.linear1(matching_inputs)\n",
    "        x = self.linear2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.linear4(x)\n",
    "        attention_weights = self.to_weight(x)\n",
    "        attention_weights = attention_weights.view(batch_size, seq_len)\n",
    "        attention_weights = F.softmax(attention_weights, dim=1)\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "216d0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoderRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(encoderRNN, self).__init__()\n",
    "        \n",
    "        self.compress = nn.Linear(4096, 512)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.gru = nn.GRU(512, 512, batch_first=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size, seq_len, feat_n = input.size()    \n",
    "        input = input.view(-1, feat_n)\n",
    "        input = self.compress(input)\n",
    "        input = self.dropout(input)\n",
    "        input = input.view(batch_size, seq_len, 512)\n",
    "\n",
    "        output, hidden_state = self.gru(input)\n",
    "\n",
    "        return output, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff05281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, vocab_size, word_dim, dropout_percentage=0.3):\n",
    "        super(decoderRNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = 512\n",
    "        self.output_size = output_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_dim = word_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, 1024)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.gru = nn.GRU(hidden_size+word_dim, hidden_size, batch_first=True)\n",
    "        self.attention = attention(hidden_size)\n",
    "        self.to_final_output = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, encoder_last_hidden_state, encoder_output, targets=None, mode='train', tr_steps=None):\n",
    "        _, batch_size, _ = encoder_last_hidden_state.size()\n",
    "        \n",
    "        decoder_current_hidden_state = None if encoder_last_hidden_state is None else encoder_last_hidden_state\n",
    "        decoder_current_input_word = Variable(torch.ones(batch_size, 1)).long()\n",
    "        #decoder_current_input_word = decoder_current_input_word.cuda()\n",
    "        seq_logProb = []\n",
    "        seq_predictions = []\n",
    "\n",
    "        targets = self.embedding(targets)\n",
    "        _, seq_len, _ = targets.size()\n",
    "\n",
    "        for i in range(seq_len-1):\n",
    "            threshold = self.teacher_forcing_ratio(training_steps=tr_steps)\n",
    "            if random.uniform(0.05, 0.995) > threshold: # returns a random float value between 0.05 and 0.995\n",
    "                current_input_word = targets[:, i]  \n",
    "            else: \n",
    "                current_input_word = self.embedding(decoder_current_input_word).squeeze(1)\n",
    "\n",
    "            context = self.attention(decoder_current_hidden_state, encoder_output)\n",
    "            gru_input = torch.cat([current_input_word, context], dim=1).unsqueeze(1)\n",
    "            gru_output, decoder_current_hidden_state = self.gru(gru_input, decoder_current_hidden_state)\n",
    "            logprob = self.to_final_output(gru_output.squeeze(1))\n",
    "            seq_logProb.append(logprob.unsqueeze(1))\n",
    "            decoder_current_input_word = logprob.unsqueeze(1).max(2)[1]\n",
    "\n",
    "        seq_logProb = torch.cat(seq_logProb, dim=1)\n",
    "        seq_predictions = seq_logProb.max(2)[1]\n",
    "        return seq_logProb, seq_predictions\n",
    "    \n",
    "    def infer(self, encoder_last_hidden_state, encoder_output):\n",
    "        _, batch_size, _ = encoder_last_hidden_state.size()\n",
    "        decoder_current_hidden_state = None if encoder_last_hidden_state is None else encoder_last_hidden_state\n",
    "        decoder_current_input_word = Variable(torch.ones(batch_size, 1)).long()\n",
    "        #decoder_current_input_word = decoder_current_input_word.cuda()\n",
    "        seq_logProb = []\n",
    "        seq_predictions = []\n",
    "        assumption_seq_len = 28\n",
    "        \n",
    "        for i in range(assumption_seq_len-1):\n",
    "            current_input_word = self.embedding(decoder_current_input_word).squeeze(1)\n",
    "            context = self.attention(decoder_current_hidden_state, encoder_output)\n",
    "            gru_input = torch.cat([current_input_word, context], dim=1).unsqueeze(1)\n",
    "            gru_output, decoder_current_hidden_state = self.gru(gru_input, decoder_current_hidden_state)\n",
    "            logprob = self.to_final_output(gru_output.squeeze(1))\n",
    "            seq_logProb.append(logprob.unsqueeze(1))\n",
    "            decoder_current_input_word = logprob.unsqueeze(1).max(2)[1]\n",
    "\n",
    "        seq_logProb = torch.cat(seq_logProb, dim=1)\n",
    "        seq_predictions = seq_logProb.max(2)[1]\n",
    "        return seq_logProb, seq_predictions\n",
    "\n",
    "    def teacher_forcing_ratio(self, training_steps):\n",
    "        return (expit(training_steps/20 +0.85)) # inverse of the logit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7e32f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MODELS(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(MODELS, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self, avi_feat, mode, target_sentences=None, tr_steps=None):\n",
    "        encoder_outputs, encoder_last_hidden_state = self.encoder(avi_feat)\n",
    "        if mode == 'train':\n",
    "            seq_logProb, seq_predictions = self.decoder(encoder_last_hidden_state = encoder_last_hidden_state, encoder_output = encoder_outputs,\n",
    "                targets = target_sentences, mode = mode, tr_steps=tr_steps)\n",
    "        elif mode == 'inference':\n",
    "            seq_logProb, seq_predictions = self.decoder.infer(encoder_last_hidden_state=encoder_last_hidden_state, encoder_output=encoder_outputs)\n",
    "        return seq_logProb, seq_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7df8fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(loss_fn, x, y, lengths):\n",
    "    batch_size = len(x)\n",
    "    predict_cat = None\n",
    "    groundT_cat = None\n",
    "    flag = True\n",
    "\n",
    "    for batch in range(batch_size):\n",
    "        predict = x[batch]\n",
    "        ground_truth = y[batch]\n",
    "        seq_len = lengths[batch] -1\n",
    "\n",
    "        predict = predict[:seq_len]\n",
    "        ground_truth = ground_truth[:seq_len]\n",
    "        if flag:\n",
    "            predict_cat = predict\n",
    "            groundT_cat = ground_truth\n",
    "            flag = False\n",
    "        else:\n",
    "            predict_cat = torch.cat((predict_cat, predict), dim=0)\n",
    "            groundT_cat = torch.cat((groundT_cat, ground_truth), dim=0)\n",
    "\n",
    "    loss = loss_fn(predict_cat, groundT_cat)\n",
    "    avg_loss = loss/batch_size\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "018eba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch(data):\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    avi_data, captions = zip(*data) \n",
    "    avi_data = torch.stack(avi_data, 0)\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "    return avi_data, targets, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abe894a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, loss_fn, parameters, optimizer, train_loader):\n",
    "    model.train()\n",
    "    print(epoch)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        avi_feats, ground_truths, lengths = batch\n",
    "        #avi_feats, ground_truths = avi_feats.cuda(), ground_truths.cuda()\n",
    "        avi_feats, ground_truths = Variable(avi_feats), Variable(ground_truths)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        seq_logProb, seq_predictions = model(avi_feats, target_sentences = ground_truths, mode = 'train', tr_steps = epoch)\n",
    "        ground_truths = ground_truths[:, 1:]  \n",
    "        loss = calculate_loss(loss_fn, seq_logProb, ground_truths, lengths)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(batch_idx%15==0):\n",
    "            print(\"b=\"+str(batch_idx))\n",
    "\n",
    "    loss = loss.item()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e4970e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(test_loader, model, i2w):\n",
    "    model.eval()\n",
    "    ss = []\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        id, avi_feats = batch\n",
    "        #avi_feats = avi_feats.cuda()\n",
    "        id, avi_feats = id, Variable(avi_feats).float()\n",
    "\n",
    "        seq_logProb, seq_predictions = model(avi_feats, mode='inference')\n",
    "        test_predictions = seq_predictions\n",
    "        \n",
    "        result = [[i2w[x.item()] if i2w[x.item()] != '<UNK>' else 'something' for x in s] for s in test_predictions]\n",
    "        result = [' '.join(s).split('<EOS>')[0] for s in result]\n",
    "        rr = zip(id, result)\n",
    "        for r in rr:\n",
    "            ss.append(r)\n",
    "    return ss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f87763c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    i2w, w2i, word_dict = data_preprocess()\n",
    "    with open('i2w.pickle', 'wb') as handle:\n",
    "        pickle.dump(i2w, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    label_file = 'HW_data/training_data/feat'\n",
    "    files_dir = 'HW_data/training_label.json'\n",
    "    train_dataset = training_data(label_file, files_dir, word_dict, w2i)\n",
    "    train_dataloader = DataLoader(dataset = train_dataset, batch_size=128, shuffle=True, num_workers=8, collate_fn=minibatch)\n",
    "    \n",
    "    epochs_n = 100\n",
    "\n",
    "    encoder = encoderRNN()\n",
    "    decoder = decoderRNN(512, len(i2w) +4, len(i2w) +4, 1024, 0.3)\n",
    "    model = MODELS(encoder=encoder, decoder=decoder)\n",
    "    \n",
    "    #model = model.cuda()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    parameters = model.parameters()\n",
    "    optimizer = optim.Adam(parameters, lr=0.0001)\n",
    "    \n",
    "    for epoch in range(epochs_n):\n",
    "        train(model, epoch+1, loss_fn, parameters, optimizer, train_dataloader) \n",
    "\n",
    "    torch.save(model, \"{}/{}.h5\".format('SavedModel', 'model0'))\n",
    "    print(\"Training finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7c07b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "406615a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "i2w, w2i, word_dict = data_preprocess()\n",
    "with open('i2w.pickle', 'wb') as handle:\n",
    "    pickle.dump(i2w, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "label_file = 'training_data/feat'\n",
    "files_dir = 'training_label.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b64c02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = training_data(label_file, files_dir, word_dict, w2i)\n",
    "train_dataloader = DataLoader(dataset = train_dataset, batch_size=60, shuffle=True, collate_fn=minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e4d7023",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_n = 10\n",
    "encoder = encoderRNN()\n",
    "decoder = decoderRNN(512, len(i2w) +4, len(i2w) +4, 1024, 0.3)\n",
    "model = MODELS(encoder=encoder, decoder=decoder)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "parameters = model.parameters()\n",
    "optimizer = optim.Adam(parameters, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64faab2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "b=0\n",
      "b=15\n",
      "b=30\n",
      "b=45\n",
      "b=60\n",
      "b=75\n",
      "b=90\n",
      "b=105\n",
      "b=120\n",
      "b=135\n",
      "b=150\n",
      "b=165\n",
      "b=180\n",
      "b=195\n",
      "b=210\n",
      "b=225\n",
      "b=240\n",
      "b=255\n",
      "b=270\n",
      "b=285\n",
      "b=300\n",
      "b=315\n",
      "b=330\n",
      "b=345\n",
      "b=360\n",
      "b=375\n",
      "b=390\n",
      "4.172874450683594\n",
      "2\n",
      "b=0\n",
      "b=15\n",
      "b=30\n",
      "b=45\n",
      "b=60\n",
      "b=75\n",
      "b=90\n",
      "b=105\n",
      "b=120\n",
      "b=135\n",
      "b=150\n",
      "b=165\n",
      "b=180\n",
      "b=195\n",
      "b=210\n",
      "b=225\n",
      "b=240\n",
      "b=255\n",
      "b=270\n",
      "b=285\n",
      "b=300\n",
      "b=315\n",
      "b=330\n",
      "b=345\n",
      "b=360\n",
      "b=375\n",
      "b=390\n",
      "3.817152976989746\n",
      "3\n",
      "b=0\n",
      "b=15\n",
      "b=30\n",
      "b=45\n",
      "b=60\n",
      "b=75\n",
      "b=90\n",
      "b=105\n",
      "b=120\n",
      "b=135\n",
      "b=150\n",
      "b=165\n",
      "b=180\n",
      "b=195\n",
      "b=210\n",
      "b=225\n",
      "b=240\n",
      "b=255\n",
      "b=270\n",
      "b=285\n",
      "b=300\n",
      "b=315\n",
      "b=330\n",
      "b=345\n",
      "b=360\n",
      "b=375\n",
      "b=390\n",
      "3.4147119522094727\n",
      "4\n",
      "b=0\n",
      "b=15\n",
      "b=30\n",
      "b=45\n",
      "b=60\n",
      "b=75\n",
      "b=90\n",
      "b=105\n",
      "b=120\n",
      "b=135\n",
      "b=150\n",
      "b=165\n",
      "b=180\n",
      "b=195\n",
      "b=210\n",
      "b=225\n",
      "b=240\n",
      "b=255\n",
      "b=270\n",
      "b=285\n",
      "b=300\n",
      "b=315\n",
      "b=330\n",
      "b=345\n",
      "b=360\n",
      "b=375\n",
      "b=390\n",
      "3.354463815689087\n",
      "5\n",
      "b=0\n",
      "b=15\n",
      "b=30\n",
      "b=45\n",
      "b=60\n",
      "b=75\n",
      "b=90\n",
      "b=105\n",
      "b=120\n",
      "b=135\n",
      "b=150\n",
      "b=165\n",
      "b=180\n",
      "b=195\n",
      "b=210\n",
      "b=225\n",
      "b=240\n",
      "b=255\n",
      "b=270\n",
      "b=285\n",
      "b=300\n",
      "b=315\n",
      "b=330\n",
      "b=345\n",
      "b=360\n",
      "b=375\n",
      "b=390\n",
      "3.2047979831695557\n",
      "6\n",
      "b=0\n",
      "b=15\n",
      "b=30\n",
      "b=45\n",
      "b=60\n",
      "b=75\n",
      "b=90\n",
      "b=105\n",
      "b=120\n",
      "b=135\n",
      "b=150\n",
      "b=165\n",
      "b=180\n",
      "b=195\n",
      "b=210\n",
      "b=225\n",
      "b=240\n",
      "b=255\n",
      "b=270\n",
      "b=285\n",
      "b=300\n",
      "b=315\n",
      "b=330\n",
      "b=345\n",
      "b=360\n",
      "b=375\n",
      "b=390\n",
      "3.0696041584014893\n",
      "7\n",
      "b=0\n",
      "b=15\n",
      "b=30\n",
      "b=45\n",
      "b=60\n",
      "b=75\n",
      "b=90\n",
      "b=105\n",
      "b=120\n",
      "b=135\n",
      "b=150\n",
      "b=165\n",
      "b=180\n",
      "b=195\n",
      "b=210\n",
      "b=225\n",
      "b=240\n",
      "b=255\n",
      "b=270\n",
      "b=285\n",
      "b=300\n",
      "b=315\n",
      "b=330\n",
      "b=345\n",
      "b=360\n",
      "b=375\n",
      "b=390\n",
      "2.8806138038635254\n",
      "8\n",
      "b=0\n",
      "b=15\n",
      "b=30\n",
      "b=45\n",
      "b=60\n",
      "b=75\n",
      "b=90\n",
      "b=105\n",
      "b=120\n",
      "b=135\n",
      "b=150\n",
      "b=165\n",
      "b=180\n",
      "b=195\n",
      "b=210\n",
      "b=225\n",
      "b=240\n",
      "b=255\n",
      "b=270\n",
      "b=285\n",
      "b=300\n",
      "b=315\n",
      "b=330\n",
      "b=345\n",
      "b=360\n",
      "b=375\n",
      "b=390\n",
      "2.746044874191284\n",
      "9\n",
      "b=0\n",
      "b=15\n",
      "b=30\n",
      "b=45\n",
      "b=60\n",
      "b=75\n",
      "b=90\n",
      "b=105\n",
      "b=120\n",
      "b=135\n",
      "b=150\n",
      "b=165\n",
      "b=180\n",
      "b=195\n",
      "b=210\n",
      "b=225\n",
      "b=240\n",
      "b=255\n",
      "b=270\n",
      "b=285\n",
      "b=300\n",
      "b=315\n",
      "b=330\n",
      "b=345\n",
      "b=360\n",
      "b=375\n",
      "b=390\n",
      "2.992175579071045\n",
      "10\n",
      "b=0\n",
      "b=15\n",
      "b=30\n",
      "b=45\n",
      "b=60\n",
      "b=75\n",
      "b=90\n",
      "b=105\n",
      "b=120\n",
      "b=135\n",
      "b=150\n",
      "b=165\n",
      "b=180\n",
      "b=195\n",
      "b=210\n",
      "b=225\n",
      "b=240\n",
      "b=255\n",
      "b=270\n",
      "b=285\n",
      "b=300\n",
      "b=315\n",
      "b=330\n",
      "b=345\n",
      "b=360\n",
      "b=375\n",
      "b=390\n",
      "2.090985059738159\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs_n):\n",
    "        train(model, epoch+1, loss_fn, parameters, optimizer, train_dataloader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8a3e549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"{}/{}.h5\".format('/Users/shreyashdhumale/Downloads', 'model0'))\n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ae1d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset.avi['rw9h_574HxE_13_18.avi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40231e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(avi('training_data/feat')['rw9h_574HxE_13_18.avi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "09da6d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dir = 'training_data/feat'\n",
    "training_feats = '' + files_dir\n",
    "files = os.listdir(training_feats)\n",
    "count = 0;\n",
    "for file in files:\n",
    "    try:\n",
    "        value = np.load(os.path.join(training_feats, file))\n",
    "        if len(value[0]) != 4096:\n",
    "            print(len(value[0]))\n",
    "    except:\n",
    "        print(file)\n",
    "        if file == \".ipynb_checkpoints\":\n",
    "            continue\n",
    "        #os.remove(training_feats+ '/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fef8c5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d03c5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
